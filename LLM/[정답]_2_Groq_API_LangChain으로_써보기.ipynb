{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MOmZwsI1kTrd"
   },
   "source": [
    "# Groq API 써보기\n",
    "\n",
    "Groq의 빠른 추론 엔진을 LangChain에서 체험하는 코드를 소개합니다.    \n",
    "langchain_groq를 통해 쉽게 연결할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "id": "daSSN5ZSetZl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\student\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.3.7)\n",
      "Collecting langchain_groq\n",
      "  Downloading langchain_groq-0.2.1-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: langchain_community in c:\\users\\student\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.3.7)\n",
      "Requirement already satisfied: pymupdf in c:\\users\\student\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.24.14)\n",
      "Requirement already satisfied: pypdf in c:\\users\\student\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (5.1.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\student\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\student\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain) (2.0.35)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\student\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain) (3.11.3)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in c:\\users\\student\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain) (0.3.19)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in c:\\users\\student\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain) (0.3.2)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in c:\\users\\student\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain) (0.1.143)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\student\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\student\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain) (2.9.2)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\student\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\student\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain) (9.0.0)\n",
      "Collecting groq<1,>=0.4.1\n",
      "  Downloading groq-0.12.0-py3-none-any.whl (108 kB)\n",
      "     ---------------------------------------- 108.9/108.9 kB ? eta 0:00:00\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\student\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in c:\\users\\student\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain_community) (0.4.0)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\users\\student\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain_community) (2.6.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\student\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\student\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\student\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\student\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\student\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\student\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\student\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.17.2)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\student\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.23.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\student\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\student\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from groq<1,>=0.4.1->langchain_groq) (4.6.2.post1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\student\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from groq<1,>=0.4.1->langchain_groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\student\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from groq<1,>=0.4.1->langchain_groq) (0.27.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\student\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from groq<1,>=0.4.1->langchain_groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\student\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from groq<1,>=0.4.1->langchain_groq) (4.12.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\student\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\student\\appdata\\roaming\\python\\python311\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain) (24.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\student\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.11)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\student\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\student\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\student\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\student\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\student\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2->langchain) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\student\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\student\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2->langchain) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\student\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\student\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\student\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\student\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\student\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain) (3.0.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\student\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
      "Installing collected packages: groq, langchain_groq\n",
      "Successfully installed groq-0.12.0 langchain_groq-0.2.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain langchain_groq langchain_community pymupdf pypdf beautifulsoup4 requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bo6qR14UoZEu"
   },
   "source": [
    "현재 사용 가능한 주요 모델과 API 제한은 아래와 같습니다.\n",
    "\n",
    "\n",
    "| ID                                     | Requests per Minute | Requests per Day | Tokens per Minute | Tokens per Day |\n",
    "|----------------------------------------|---------------------|------------------|-------------------|----------------|\n",
    "| gemma-7b-it                            | 30                  | 14,400           | 15,000            | 500,000        |\n",
    "| gemma2-9b-it                           | 30                  | 14,400           | 15,000            | 500,000        |\n",
    "| llama-3.1-70b-versatile                | 30                  | 14,400           | 18,000            | 500,000        |\n",
    "| llama-3.1-8b-instant                   | 30                  | 14,400           | 20,000            | 500,000        |\n",
    "| llama-3.2-11b-text-preview             | 30                  | 7,000            | 7,000             | 500,000        |\n",
    "| llama-3.2-11b-vision-preview           | 30                  | 7,000            | 7,000             | 500,000        |\n",
    "| llama-3.2-1b-preview                   | 30                  | 7,000            | 7,000             | 500,000        |\n",
    "| llama-3.2-3b-preview                   | 30                  | 7,000            | 7,000             | 500,000        |\n",
    "| llama-3.2-90b-text-preview             | 30                  | 7,000            | 7,000             | 500,000        |\n",
    "| llama-3.2-90b-vision-preview           | 15                  | 3,500            | 7,000             | 250,000        |\n",
    "| llava-v1.5-7b-4096-preview             | 30                  | 14,400           | 30,000            | (No limit)     |\n",
    "| mixtral-8x7b-32768                     | 30                  | 14,400           | 5,000             | 500,000        |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ZozwH3G_kPDc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq # Groq-LangChain 연결\n",
    "\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "# from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TcNUf5TCpZTe"
   },
   "source": [
    "간단한 체인을 만들고 실행합니다.\n",
    "\n",
    "https://console.groq.com/keys 에서 키를 생성해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "NVj9S2kZeThO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groq의 LPU (Language Processing Unit) 기술 덕분에 빠른 추론 속도를 제공합니다. LPU는 기존의 그래픽 처리를 위한 GPU와 달리, AI 추론과 언어 처리를 위해 특별히 설계되었습니다. 이로 인해 LPU는 기존의 GPU보다 더 빠른 추론 속도와 더 나은 에너지 효율성을 제공할 수 있습니다.\n",
      "Elapsed Time: 0.8567409515380859 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['GROQ_API_KEY'] = ''\n",
    "\n",
    "\n",
    "chat = ChatGroq(\n",
    "    temperature=0.1,\n",
    "    model=\"llama-3.1-70b-versatile\",\n",
    ")\n",
    "\n",
    "url = \"https://wow.groq.com/why-groq/\"\n",
    "loader = WebBaseLoader(url)\n",
    "\n",
    "docs = loader.load()\n",
    "question = \"Groq 엔진의 LLM 추론 속도가 빠른 이유는 무엇인가요? 한국어로 답변하세요.\"\n",
    "\n",
    "\n",
    "system = \"Answer the question from given contexts. Answer in Korean.\"\n",
    "human = \"\"\"\n",
    "Context: {context}\n",
    "\n",
    "---\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", human)])\n",
    "\n",
    "chain = prompt | chat | StrOutputParser()\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "result = chain.invoke(\n",
    "          {\"context\":docs[0].page_content,\n",
    "            'question':question})\n",
    "print(result)\n",
    "\n",
    "end = time.time()\n",
    "elapsed_time = end - start\n",
    "print(f\"Elapsed Time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UyM4Yt-fMpoX"
   },
   "source": [
    "## Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A4F1k171MpoY"
   },
   "source": [
    "요약은 LLM의 아주 중요한 기능 중 하나입니다.   \n",
    "일반적으로, LLM의 Abstractive Summarization은 3개의 방법을 사용합니다.\n",
    "\n",
    "- Stuff : 전체 코퍼스를 하나의 프롬프트에 넣고 요약 생성하기\n",
    "- Map-Reduce : 코퍼스를 청크로 분리하고, 각 청크의 요약을 생성한 뒤 합치기\n",
    "- Refine: 코퍼스를 청크로 분리하고, 순차적으로 읽으며 요약 업데이트하기\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w9vkYtYkMpoa"
   },
   "source": [
    "## Stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CUemn2D7ikKc"
   },
   "source": [
    "gemma 2 모델을 이용해 요약을 수행해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "kMmlgYzWMpoa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16713\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Summarize the following paper in Korean.\\nEmphasize the uniqueness and contribution of the paper.\\nAnswer should be in 20 sentences.\\nPlease Answer in Korean.\\n', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='\\n\\nLlama 3.2: Revolutionizing edge AI and vision with open, customizable models\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nOur approachResearchProduct experiencesLlamaBlogTry Meta AIFEATUREDLarge Language Model Llama 3.2: Revolutionizing edge AI and vision with open, customizable modelsSeptember 25, 2024•15 minute readTakeaways:Today, we’re releasing Llama 3.2, which includes small and medium-sized vision LLMs (11B and 90B), and lightweight, text-only models (1B and 3B) that fit onto edge and mobile devices, including pre-trained and instruction-tuned versions.The Llama 3.2 1B and 3B models support context length of 128K tokens and are state-of-the-art in their class for on-device use cases like summarization, instruction following, and rewriting tasks running locally at the edge. These models are enabled on day one for Qualcomm and MediaTek hardware and optimized for Arm processors.Supported by a broad ecosystem, the Llama 3.2 11B and 90B vision models are drop-in replacements for their corresponding text model equivalents, while exceeding on image understanding tasks compared to closed models, such as Claude 3 Haiku. Unlike other open multimodal models, both pre-trained and aligned models are available to be fine-tuned for custom applications using torchtune and deployed locally using torchchat. They’re also available to try using our smart assistant, Meta AI.We’re sharing the first official Llama Stack distributions, which will greatly simplify the way developers work with Llama models in different environments, including single-node, on-prem, cloud, and on-device, enabling turnkey deployment of retrieval-augmented generation (RAG) and tooling-enabled applications with integrated safety.We’ve been working closely with partners like AWS, Databricks, Dell Technologies, Fireworks, Infosys, and Together AI to build Llama Stack distributions for their downstream enterprise clients. On-device distribution is via PyTorch ExecuTorch, and single-node distribution is via Ollama.We continue to share our work because we believe openness drives innovation and is good for developers, Meta, and the world. Llama is already leading the way on openness, modifiability, and cost efficiency—enabling more people to have creative, useful, and life-changing breakthroughs using generative AI.We’re making Llama 3.2 models available for download on llama.com and Hugging Face, as well as available for immediate development on our broad ecosystem of partner platforms, including AMD, AWS, Databricks, Dell, Google Cloud, Groq, IBM, Intel, Microsoft Azure, NVIDIA, Oracle Cloud, Snowflake, and more.We’ve been excited by the impact the Llama 3.1 herd of models have made in the two months since we announced them, including the 405B—the first open frontier-level AI model. While these models are incredibly powerful, we recognize that building with them requires significant compute resources and expertise. We’ve also heard from developers who don’t have access to these resources and still want the opportunity to build with Llama. As Meta Founder and CEO Mark Zuckerberg shared today at Connect, they won’t have to wait any longer. Today, we’re releasing Llama 3.2, which includes small and medium-sized vision LLMs (11B and 90B) and lightweight, text-only models (1B and 3B) that fit onto select edge and mobile devices.It’s only been a year and a half since we first announced Llama, and we’ve made incredible progress in such a short amount of time. This year, Llama has achieved 10x growth and become the standard for responsible innovation. Llama also continues to lead on openness, modifiability, and cost efficiency, and it’s competitive with closed models—even leading in some areas. We believe that openness drives innovation and is the right path forward, which is why we continue to share our research and collaborate with our partners and the developer community.We’re making Llama 3.2 models available for download on llama.com and Hugging Face, as well as available for immediate development on our broad ecosystem of partner platforms. Partners are an important part of this work, and we’ve worked with over 25 companies, including AMD, AWS, Databricks, Dell, Google Cloud, Groq, IBM, Intel, Microsoft Azure, NVIDIA, Oracle Cloud, and Snowflake, to enable services on day one. For the Llama 3.2 release, we’re also working with on-device partners Arm, MediaTek, and Qualcomm to offer a broad range of services at launch. Starting today, we’re also making Llama Stack available to the community. More details on the latest release, including information on the multimodal availability in Europe, can be found in our acceptable use policy.Meet Llama 3.2The two largest models of the Llama 3.2 collection, 11B and 90B, support image reasoning use cases, such as document-level understanding including charts and graphs, captioning of images, and visual grounding tasks such as directionally pinpointing objects in images based on natural language descriptions. For example, a person could ask a question about which month in the previous year their small business had the best sales, and Llama 3.2 can then reason based on an available graph and quickly provide the answer. In another example, the model could reason with a map and help answer questions such as when a hike might become steeper or the distance of a particular trail marked on the map. The 11B and 90B models can also bridge the gap between vision and language by extracting details from an image, understanding the scene, and then crafting a sentence or two that could be used as an image caption to help tell the story.The lightweight 1B and 3B models are highly capable with multilingual text generation and tool calling abilities. These models empower developers to build personalized, on-device agentic applications with strong privacy where data never leaves the device. For example, such an application could help summarize the last 10 messages received, extract action items, and leverage tool calling to directly send calendar invites for follow-up meetings.Running these models locally comes with two major advantages. First, prompts and responses can feel instantaneous, since processing is done locally. Second, running models locally maintains privacy by not sending data such as messages and calendar information to the cloud, making the overall application more private. Since processing is handled locally, the application can clearly control which queries stay on the device and which may need to be processed by a larger model in the cloud.Model evaluationsOur evaluation suggests that the Llama 3.2 vision models are competitive with leading foundation models, Claude 3 Haiku and GPT4o-mini on image recognition and a range of visual understanding tasks. The 3B model outperforms the Gemma 2 2.6B and Phi 3.5-mini models on tasks such as following instructions, summarization, prompt rewriting, and tool-use, while the 1B is competitive with Gemma.We evaluated performance on over 150 benchmark datasets that span a wide range of languages. For the vision LLMs, we evaluated performance on benchmarks for image understanding and visual reasoning.Vision modelsAs the first Llama models to support vision tasks, the 11B and 90B models required an entirely new model architecture that supports image reasoning.To add image input support, we trained a set of adapter weights that integrate the pre-trained image encoder into the pre-trained language model. The adapter consists of a series of cross-attention layers that feed image encoder representations into the language model. We trained the adapter on text-image pairs to align the image representations with the language representations. During adapter training, we also updated the parameters of the image encoder, but intentionally did not update the language-model parameters. By doing that, we keep all the text-only capabilities intact, providing developers a drop-in replacement for Llama 3.1 models.Our training pipeline consists of multiple stages, starting from pretrained Llama 3.1 text models. First, we add image adapters and encoders, then pretrain on large-scale noisy (image, text) pair data. Next, we train on medium-scale high quality in-domain and knowledge-enhanced (image, text) pair data.In post-training, we use a similar recipe as the text models by doing several rounds of alignment on supervised fine-tuning, rejection sampling, and direct preference optimization. We leverage synthetic data generation by using the Llama 3.1 model to filter and augment question and answers on top of in-domain images, and use a reward model to rank all the candidate answers to provide high quality fine-tuning data. We also add safety mitigation data to produce a model with a high level of safety while retaining helpfulness of the modeThe end result is a set of models that can take in both image and text prompts, and deeply understand and reason on the combination. This is another step toward Llama models having even richer agentic capabilities.Lightweight modelsAs we talked about with Llama 3.1, powerful teacher models can be leveraged to create smaller models that have improved performance. We used two methods—pruning and distillation—on the 1B and 3B models, making them the first highly capable lightweight Llama models that can fit on devices efficiently.Pruning enabled us to reduce the size of extant models in the Llama herd while recovering as much knowledge and performance as possible. For the 1B and 3B models, we took the approach of using structured pruning in a single shot manner from the Llama 3.1 8B. This involved systematically removing parts of the network and adjusting the magnitude of the weights and gradients to create a smaller, more efficient model that retains the performance of the original network.Knowledge distillation uses a larger network to impart knowledge on a smaller network, with the idea that a smaller model can achieve better performance using a teacher than it could from scratch. For the 1B and 3B in Llama 3.2, we incorporated logits from the Llama 3.1 8B and 70B models into the pre-training stage of the model development, where outputs (logits) from these larger models were used as token-level targets. Knowledge distillation was used after pruning to recover performance.In post-training, we use a similar recipe as Llama 3.1 and produce final chat models by doing several rounds of alignment on top of the pre-trained model. Each round involves supervised fine-tuning (SFT), rejection sampling (RS), and direct preference optimization (DPO).In post-training, we scale context length support to 128K tokens, while maintaining the same quality as the pre-trained model. We also engage in synthetic data generation that goes through careful data processing and filtering to ensure high quality. We carefully blend the data to optimize for high quality across multiple capabilities like summarization, rewriting, instruction following, language reasoning, and tool use.To enable the community to innovate on these models, we worked closely with Qualcomm and Mediatek, the top two mobile system on a chip (SoC) companies in the world, and Arm, who provides the foundational compute platform for 99% of mobile devices. The weights being released today are based on BFloat16 numerics. Our teams are actively exploring quantized variants that will run even faster, and we hope to share more on that soon.This demo is based on an unreleased quantized model.This demo is based on an unreleased quantized model.Llama Stack distributionsIn July, we released a request for comment on the Llama Stack API, a standardized interface for canonical toolchain components (fine-tuning, synthetic data generation) to customize Llama models and build agentic applications. The engagement has been great.Since then, we have been working hard to make the API real. We built a reference implementation of the APIs for inference, tool use, and RAG. In addition, we have been working with partners to adapt them to become providers for the APIs. Finally, we have introduced Llama Stack Distribution as a way to package multiple API Providers that work well together to provide a single endpoint for developers. We are now sharing with the community a simplified and consistent experience that will enable them to work with Llama models in multiple environments, including on-prem, cloud, single-node, and on-device.The full set of releases includes:Llama CLI (command line interface) to build, configure, and run Llama Stack distributionsClient code in multiple languages, including python, node, kotlin, and swiftDocker containers for Llama Stack Distribution Server and Agents API ProviderMultiple distributionsSingle-node Llama Stack Distribution via Meta internal implementation and OllamaCloud Llama Stack distributions via AWS, Databricks, Fireworks, and TogetherOn-device Llama Stack Distribution on iOS implemented via PyTorch ExecuTorchOn-prem Llama Stack Distribution supported by DellWe look forward to working with developers and partners to simplify all aspects of building with Llama models and welcome feedback.System level safetyTaking an open approach has many benefits. It helps ensure that more people around the world can access the opportunities that AI provides, guards against concentrating power in the hands of a small few, and deploys technology more equitably and safely across society. As we continue to innovate, we also want to make sure we’re empowering developers to build safe and responsible systems.Building on our previous release and continuous effort to support responsible innovation, today we’re adding new updates to our family of safeguards:First, we’re releasing Llama Guard 3 11B Vision, which is designed to support Llama 3.2’s new image understanding capability and filter text+image input prompts or text output responses to these prompts.Second, as we released 1B and 3B Llama models to be used in more constrained environments like on-device, we also optimized Llama Guard to drastically reduce its deployment cost. Llama Guard 3 1B is based on the Llama 3.2 1B model and has been pruned and quantized bringing its size from 2,858 MB down to 438 MB, making it more efficient than ever to deploy.These new solutions are integrated into our reference implementations, demos, and applications and are ready for the open source community to use on day one.Try Llama 3.2 todayLlama 3.2 is poised to reach more people than ever before and enable exciting new use cases. We believe sharing these models with the open source community isn’t enough. We want to make sure developers also have the tools they need to build with Llama responsibly. As part of our continued responsible release efforts, we’re offering developers new tools and resources, and as always, we’ll update best practices in our Responsible Use Guide.We continue to share the latest advancements in the Llama ecosystem because we believe openness drives innovation and is good for developers, Meta, and the world. We’re excited to continue the conversations we’re having with our partners and the open source community, and as always, we can’t wait to see what the community builds using Llama 3.2 and Llama Stack.This work was supported by our partners across the AI community. We’d like to thank and acknowledge (in alphabetical order): Accenture, AMD, Arm, AWS, Cloudflare, Databricks, Dell, Deloitte, Fireworks.ai, Google Cloud, Groq, Hugging Face, IBM watsonx, Infosys, Intel, Kaggle, Lenovo, LMSYS, MediaTek, Microsoft Azure, NVIDIA, OctoAI, Ollama, Oracle Cloud, PwC, Qualcomm, Sarvam AI, Scale AI, Snowflake, Together AI, and UC Berkeley - vLLM Project.Learn more on the Llama websiteVisit Hugging FaceShare:Our latest updates delivered to your inboxSubscribe to our newsletter to keep up with Meta AI news, events, research breakthroughs, and more.Join us in the pursuit of what’s possible with AI.See all open positionsRelated PostsResponsible AIConnect 2024: The responsible approach we’re taking to generative AI September 25, 2024Read postWith 10x growth since 2023, Llama is the leading engine of AI innovationAugust 29, 2024Read postOpen SourceGenerate an entire app from a prompt using Together AI’s LlamaCoderSeptember 18, 2024Read postOur approachAbout AI at MetaResponsibilityPeopleCareersResearchInfrastructureResourcesDemosProduct experiencesMeta AIAI StudioLatest newsBlogNewsletterFoundational modelsLlamaOur approachOur approachAbout AI at MetaResponsibilityPeopleCareersResearchResearchInfrastructureResourcesDemosProduct experiencesMeta AIAI StudioLatest newsLatest newsBlogNewsletterFoundational modelsLlamaPrivacy PolicyTermsCookies Meta © 2024\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stuff\n",
    "example_URL='https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/'\n",
    "\n",
    "loader = WebBaseLoader(example_URL)\n",
    "docs = loader.load()\n",
    "\n",
    "\n",
    "gemma2 = ChatGroq(\n",
    "    temperature=0.2,\n",
    "    model=\"gemma2-9b-it\",\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "summarize_prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', '''Summarize the following paper in Korean.\n",
    "Emphasize the uniqueness and contribution of the paper.\n",
    "Answer should be in 20 sentences.\n",
    "Please Answer in Korean.\n",
    "'''),\n",
    "    ('user', '''{text}''')])\n",
    "\n",
    "print(len(docs[0].page_content))\n",
    "summarize_prompt.format_messages(text=docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "5MOx1k71Mpoa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Llama 3.2: 핵심 내용 요약 및 차별성 강조 (20문장)\n",
      "\n",
      "Meta는 오픈소스, 맞춤형 모델인 Llama 3.2를 출시하여 에지 AI 및 비전 분야에 혁신을 가져올 것이라고 발표했습니다. Llama 3.2는 11B 및 90B 크기의 비전 LLM과 1B 및 3B 크기의 가벼운 텍스트 전용 모델을 포함하며, Qualcomm 및 MediaTek 하드웨어와 Arm 프로세서를 위한 최적화된 버전을 제공합니다. \n",
      "\n",
      "이 모델은 텍스트 및 이미지를 이해하고 처리할 수 있는 능력을 갖추고 있으며, 이는 문서 이해, 이미지 설명, 이미지 기반 질문 답변 등 다양한 비전 작업에 적용될 수 있습니다. 특히, Llama 3.2는 텍스트 모델과 비슷한 성능을 유지하면서도 이미지 인코더를 통합하여 새로운 비전 작업을 수행할 수 있도록 설계되었습니다.\n",
      "\n",
      "Llama 3.2의 주요 특징은 다음과 같습니다:\n",
      "\n",
      "* **오픈소스 및 맞춤형:** 개발자들이 모델을 자유롭게 사용하고 수정할 수 있도록 지원합니다.\n",
      "* **에지 및 모바일 기기 지원:** 1B 및 3B 모델은 에지 및 모바일 기기에 적합한 크기로 설계되었습니다.\n",
      "* **강력한 비전 이해 능력:** 11B 및 90B 모델은 이미지 인식, 이해, 그리고 설명을 위한 강력한 능력을 제공합니다.\n",
      "* **다양한 환경 지원:** Llama Stack를 통해 다양한 환경 (온프레미스, 클라우드, 단일 노드, 에지)에서 모델을 사용할 수 있습니다.\n",
      "* **안전성 강화:** Llama Guard 3를 통해 안전하고 책임감 있는 AI 개발을 지원합니다.\n",
      "\n",
      "Llama 3.2는 기존 Llama 모델보다 더욱 강력하고 다용도적인 모델이며, 오픈소스 접근 방식을 통해 AI 기술 접근성을 높이고 혁신을 촉진할 것으로 기대됩니다.\n",
      "\n",
      "특히, Llama 3.2는 다음과 같은 측면에서 차별성을 가지고 있습니다:\n",
      "\n",
      "* **비전 모델 지원:** Llama 3.2는 이미지 처리 능력을 갖춘 최초의 Llama 모델로, 비전 분야에서 새로운 가능성을 열었습니다.\n",
      "* **에지 및 모바일 기기 최적화:** 1B 및 3B 모델은 에지 및 모바일 기기에서도 효율적으로 작동할 수 있도록 최적화되었습니다.\n",
      "* **Llama Stack 제공:** Llama Stack를 통해 모델 사용을 더욱 간편하고 효율적으로 만들었습니다.\n",
      "* **안전성 강화:** Llama Guard 3를 통해 안전하고 책임감 있는 AI 개발을 지원합니다.\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summarize_chain = summarize_prompt | gemma2 | StrOutputParser()\n",
    "summary = summarize_chain.invoke(docs[0].page_content)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZCaANqOKMpoa"
   },
   "source": [
    "해당 방법은 매우 간단하지만, Context 길이를 넘어서는 경우 에러가 발생합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nLO3VmBdikKd"
   },
   "source": [
    "## Map-Reduce\n",
    "LangChain의 PyMuPdfLoader를 이용하여 임의의 PDF를 요약해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "14r339tyikKd"
   },
   "outputs": [],
   "source": [
    "# # Password 있는 PDF 열기\n",
    "# from langchain.document_loaders import PyPDFLoader\n",
    "# path_material = './암호화된파일.pdf'\n",
    "\n",
    "# pypdf_loader = PyPDFLoader(path_material, password='비밀번호')\n",
    "# material_pages = pypdf_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "BqWN9mrkikKd"
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "import urllib.request\n",
    "\n",
    "paper_URL = \"https://arxiv.org/pdf/2410.05983\"\n",
    "\n",
    "\n",
    "# 외부 링크에서 PDF 파일을 다운로드하는 코드\n",
    "urllib.request.urlretrieve(\n",
    "    paper_URL,\n",
    "    filename=\"paper.pdf\"\n",
    ")\n",
    "path = './paper.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ImhjYgkqikKd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97365"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "loader = PyMuPDFLoader(path)\n",
    "# 페이지별로 저장\n",
    "pages = loader.load()\n",
    "\n",
    "# 코퍼스에 모두 결합\n",
    "corpus = Document(page_content='')\n",
    "for page in pages:\n",
    "    corpus.page_content += page.page_content\n",
    "len(corpus.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YvhDiGhdMpoa"
   },
   "source": [
    "긴 Context를 처리하기 위해, 전체 코퍼스를 작은 단위로 쪼개 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "c5pXmAy-Mpoa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=1000)\n",
    "document_list = text_splitter.split_documents([corpus])\n",
    "len(document_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UE34hmK8Mpoa"
   },
   "source": [
    "이후 Map-Reduce와 Refine을 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D1rP6f8OMpoa"
   },
   "source": [
    "## Map-Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "itfSciDVMpoa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 1/11 [00:01<00:13,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# 0\n",
      "본 논문은 장문 맥락을 처리할 수 있는 대규모 언어 모델(LLM)을 기반으로 한 정보 검색 강화 생성(RAG) 시스템의 새로운 과제와 해결책을 제시합니다. \n",
      "\n",
      "장문 맥락 LLM을 RAG에 적용할 때, 단순히 검색된 문서의 개수를 늘리는 것이 성능 향상을 보장하지 않는다는 것을 발견했습니다. 오히려 검색된 문서의 개수가 증가함에 따라 성능이 감소하는 경우도 있었습니다. 이는 검색된 문서 중에서도 관련성이 낮거나 오히려 오류를 유발할 수 있는 \"hard negatives\"가 LLM의 생성 과정에 부정적인 영향을 미치기 때문입니다. \n",
      "\n",
      "논문에서는 이러한 문제를 해결하기 위해 \"retrieval reordering\", \"implicit robustness fine-tuning\", \"explicit relevance fine-tuning\" 세 가지 방법을 제안합니다. 첫 번째 방법은 검색된 문서의 순서를 재정렬하여 LLM이 더욱 관련성 있는 정보에 집중하도록 유도합니다. 두 번째 방법은 LLM을 노이즈가 포함된 데이터로 fine-tuning하여 hard negatives에 대한 내성을 향상시킵니다. 마지막 방법은 LLM이 검색된 문서를 분석하고 관련 정보를 명시적으로 식별하도록 학습시켜 LLM의 정보 필터링 능력을 강화합니다. \n",
      "\n",
      "이러한 방법들은 장문 맥락 LLM을 기반으로 한 RAG 시스템의 성능을 향상시키는 데 효과적임을 보여줍니다. 또한, 데이터 분포, 사용된 검색 알고리즘, 학습 맥락 길이와 같은 RAG-specific LLM fine-tuning에 영향을 미치는 요소들을 분석하여 실질적인 지침을 제공합니다.\n",
      "\n",
      "\n",
      "\n",
      "===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 2/11 [00:02<00:09,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# 1\n",
      "본 논문은 장문맥 LLM(Long-Context LLMs)을 기반으로 한 RAG(Retrieval-Augmented Generation) 시스템에서 발생하는 과제들을 분석하고 해결 방안을 제시합니다. \n",
      "\n",
      "첫째, 장문맥 LLM이 처리할 수 있는 답변 텍스트의 길이가 증가함에 따라 RAG 성능이 증가하지만, 특정 지점 이후에는 감소하는 경향을 보인다는 것을 발견했습니다. 이는 강력한 검색기(retriever)가 더 많은 관련 텍스트를 검색하지만, 이 중 일부는 실제로는 관련성이 낮거나 오히려 오답을 유발하는 \"hard negatives\"를 포함할 수 있기 때문입니다. 둘째, 높은 정확도를 보이는 검색기가 \"hard negatives\"를 더 많이 검색하여 LLM 성능에 더 큰 부정적인 영향을 미칠 수 있다는 것을 알 수 있습니다. 즉, 단순히 검색 결과의 양보다 검색 결과의 질, 특히 \"hard negatives\"의 양과 특성이 LLM 성능에 큰 영향을 미친다는 것을 시사합니다. \n",
      "\n",
      "이러한 연구 결과를 바탕으로, 장문맥 LLM을 RAG에 적용할 때 \"hard negatives\"를 효과적으로 처리하고 LLM이 검색된 정보를 효율적으로 활용할 수 있도록 하는 새로운 방법론 개발이 필요함을 제시합니다.\n",
      "\n",
      "\n",
      "\n",
      "===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 3/11 [00:03<00:09,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# 2\n",
      "본 논문은 긴 텍스트 컨텍스트에서의 RAG(Retrieval Augmented Generation) 시스템에서 긴 텍스트 컨텍스트를 가진 LLM(Large Language Model)의 성능을 향상시키는 방법을 연구합니다. \n",
      "\n",
      "첫째, 논문은 기존의 평가 지표인 정확도가 긴 텍스트 컨텍스트에서의 LLM 성능을 충분히 반영하지 못한다는 점을 보여줍니다. 특히, \"hard negatives\" (참과 거짓을 구분하기 어려운 관련 없는 텍스트)가 LLM의 성능에 큰 영향을 미치며, 이를 고려한 새로운 평가 방법론이 필요함을 강조합니다. \n",
      "\n",
      "둘째, 논문은 \"retrieval reordering\"이라는 쉬운 방법을 제시합니다. 이 방법은 LLM이 긴 텍스트 컨텍스트에서 \"hard negatives\"의 영향을 덜 받도록, 검색된 텍스트의 순서를 재정렬하는 것입니다. 실험 결과, retrieval reordering은 특히 많은 텍스트가 검색되었을 때 LLM의 성능을 향상시키는 데 효과적임을 보여줍니다.\n",
      "\n",
      "셋째, 논문은 RAG에 특화된 fine-tuning 방법을 통해 LLM의 \"hard negatives\"에 대한 내성을 향상시킬 수 있다고 제안합니다. 이 방법은 LLM을 다양한 컨텍스트에서 학습시켜 \"hard negatives\"를 효과적으로 식별하고 처리할 수 있도록 돕습니다.\n",
      "\n",
      "\n",
      "요약하자면, 본 논문은 긴 텍스트 컨텍스트에서의 RAG 시스템에서 LLM의 성능을 향상시키기 위한 새로운 접근 방식을 제시합니다. 특히, \"hard negatives\"의 문제점을 인식하고 이를 해결하기 위한 retrieval reordering과 RAG-specific fine-tuning 방법을 제안하며, 이를 통해 LLM의 실제 응용 환경에서의 성능을 향상시킬 수 있음을 보여줍니다. \n",
      "\n",
      "===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▋      | 4/11 [00:04<00:07,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# 3\n",
      "본 논문은 Long-Context LLM을 RAG(Retrieval Augmented Generation)에 적용하여 성능을 향상시키는 방법을 제시합니다. \n",
      "\n",
      "첫째, 논문은 RAG-specific fine-tuning을 통해 LLM의 Robustness를 향상시키는 방법을 소개합니다. 이 방법은 LLM을 instruction, query, retrieved passages를 입력으로 받아 정답을 생성하도록 학습시키는 방식으로, 다양한 retrieved context를 학습하여 hard negatives에 대한 저항력을 높입니다. 실험 결과, RAG-specific fine-tuning은 기존의 chat model과 Direct fine-tuning보다 모든 데이터셋에서 우수한 성능을 보였습니다.\n",
      "\n",
      "둘째, 논문은 intermediate reasoning을 추가하여 LLM의 relevance identification 능력을 향상시키는 방법을 제시합니다. 이 방법은 LLM이 reasoning paragraph를 생성하여 relevant passages를 명시적으로 식별하도록 학습시키는 방식으로, LLM이 retrieved context에서 중요한 정보를 효과적으로 추출하도록 돕습니다. 실험 결과, intermediate reasoning을 추가한 RAG-specific fine-tuning은 기존의 RAG-specific fine-tuning보다 더 높은 성능을 보였습니다.\n",
      "\n",
      "셋째, 논문은 RAG fine-tuning에 사용되는 데이터 분포, retriever, training context length의 영향을 분석합니다. 다양한 데이터 분포, 여러 retriever를 사용한 fine-tuning, 최대 context length로 fine-tuning하는 것이 LLM의 성능 향상에 효과적임을 확인했습니다.\n",
      "\n",
      "결론적으로, 본 논문은 RAG에 적용된 Long-Context LLM의 성능을 향상시키기 위한 다양한 방법을 제시하고, 실험 결과를 통해 그 효과를 입증합니다.\n",
      "\n",
      "\n",
      "\n",
      "===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 5/11 [00:05<00:07,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# 4\n",
      "본 논문은 장 문맥 LLM이 Retrieval-Augmented Generation(RAG) 시스템에서 장 문맥을 처리하는 데 어려움을 겪는 문제를 해결하기 위해 다양한 접근 방식을 제안하고 평가합니다. \n",
      "\n",
      "첫째, 논문은 여러 개의 검색된 문장을 포함할수록 성능이 향상되지만, 특정한 \"hard negatives\"로 인해 성능이 저하되는 현상을 발견했습니다. 둘째, 이 문제를 해결하기 위해 검색된 문장의 순서를 재정렬하는 방법, RAG에 특화된 암묵적 LLM fine-tuning, 그리고 중간 추론을 포함하는 RAG-oriented LLM fine-tuning을 제안했습니다. 셋째, 데이터 분포, 훈련에 사용되는 검색기, 훈련 문맥 길이 등의 요소가 훈련된 LLM의 성능에 미치는 영향을 분석했습니다. \n",
      "\n",
      "결론적으로, 논문은 장 문맥 LLM을 RAG에 적용할 때 발생하는 어려움을 해결하기 위한 새로운 방법을 제시하고, 이러한 방법들이 RAG 성능을 향상시킬 수 있음을 보여줍니다. \n",
      "\n",
      "특히, 논문은 다음과 같은 핵심 내용을 다룹니다.\n",
      "\n",
      "* 장 문맥 LLM이 RAG에서 장 문맥을 처리하는 데 어려움을 겪는 문제점을 제시합니다.\n",
      "* 여러 개의 검색된 문장을 포함할수록 성능이 향상되지만, \"hard negatives\"로 인해 성능이 저하되는 현상을 발견합니다.\n",
      "* 검색된 문장의 순서 재정렬, RAG-특화 암묵적 fine-tuning, 중간 추론을 포함하는 RAG-oriented fine-tuning을 제안합니다.\n",
      "* 데이터 분포, 검색기, 훈련 문맥 길이 등이 훈련된 LLM의 성능에 미치는 영향을 분석합니다.\n",
      "* 제안된 방법들이 RAG 성능을 향상시킬 수 있음을 보여줍니다.\n",
      "\n",
      "\n",
      "\n",
      "===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 6/11 [00:18<00:24,  4.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# 5\n",
      "본 논문은 장문 맥락을 이해하는 대규모 언어 모델(LLM)이 Retrieval Augmented Generation(RAG)과 같은 기존 질의응답 시스템과 어떻게 상호 작용하는지 연구합니다. \n",
      "\n",
      "특히, 장문 맥락 LLM이 RAG에 적용될 때 발생하는 새로운 과제들을 분석하고, 이러한 과제들을 극복하기 위한 방법들을 제시합니다. \n",
      "\n",
      "첫째, 장문 맥락 LLM이 많은 양의 검색 결과를 처리할 때 오히려 성능이 저하될 수 있다는 것을 발견했습니다. 이는 검색 결과 중에서 관련성이 낮거나 오히려 오답을 제공하는 정보가 LLM의 성능을 방해하기 때문입니다. \n",
      "\n",
      "둘째, 장문 맥락 LLM은 강력한 검색 엔진이 제공하는 검색 결과에 더 취약하다는 것을 알 수 있습니다. 이는 강력한 검색 엔진이 더 많은 관련성이 높은 정보를 찾을 수 있지만, 동시에 더 많은 오답 정보도 제공할 수 있기 때문입니다. \n",
      "\n",
      "셋째, 논문은 장문 맥락 LLM의 성능을 평가할 때 \"hard negative\" (정답이 아닌 관련성이 높은 오답)를 고려해야 함을 강조합니다. \n",
      "\n",
      "마지막으로, 논문은 이러한 문제들을 해결하기 위해 다양한 방법을 제안합니다. 예를 들어, 검색 결과의 품질을 향상시키거나, LLM이 오답 정보를 효과적으로 필터링할 수 있도록 학습시키는 방법이 있습니다. \n",
      "\n",
      "이러한 연구는 장문 맥락 LLM을 RAG에 적용하는 데 있어서 중요한 과제들을 제시하며, 향후 LLM 기반 질의응답 시스템의 발전에 기여할 것으로 기대됩니다.\n",
      "\n",
      "\n",
      "\n",
      "===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▎   | 7/11 [00:34<00:34,  8.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# 6\n",
      "본 논문은 긴 텍스트 입력에 대한 Retrieval Augmented Generation(RAG) 시스템의 성능을 향상시키기 위한 방법을 제시합니다. 특히, 긴 맥락을 이해하는 대규모 언어 모델(LLM)을 RAG에 적용했을 때 발생하는 어려움을 해결하는 데 초점을 맞춥니다. \n",
      "\n",
      "첫째, 논문은 LLM이 긴 입력에 대해 효율적으로 학습하고 처리하기 위해 텍스트를 효과적으로 요약하는 방법을 제시합니다. 둘째, 논문은 다양한 Retriever를 사용하여 관련성이 높은 텍스트를 추출하는 방법을 비교 분석하고, LLM의 성능 향상에 가장 효과적인 Retriever를 선택하는 방법을 제안합니다. 셋째, 논문은 LLM이 훈련 데이터에서 학습한 지식을 바탕으로 새로운 질문에 대한 답변을 생성하는 과정에서 발생하는 오류를 줄이기 위한 방법을 제시합니다. 넷째, 논문은 LLM이 긴 텍스트에서 중요한 정보를 추출하고 이해하는 능력을 향상시키기 위한 새로운 기술을 연구합니다. 다섯째, 논문은 LLM을 RAG에 적용했을 때 발생하는 윤리적 문제점을 논의하고, 이러한 문제점을 해결하기 위한 방안을 제시합니다. 마지막으로, 논문은 LLM 기반 RAG 시스템의 미래 전망을 제시하고, 향후 연구 방향을 제시합니다.\n",
      "\n",
      "\n",
      "\n",
      "===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 8/11 [00:45<00:28,  9.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# 7\n",
      "본 논문은 긴 입력에 대한 RAG(Retrieval Augmented Generation)의 과제를 해결하기 위해 긴 맥락 LLM(Long-Context Large Language Model)을 사용하는 방법을 제시합니다. \n",
      "\n",
      "1. 논문은 긴 입력을 처리하는 RAG 모델을 개발하기 위한 다양한 접근 방식을 소개합니다. \n",
      "2. 특히, 긴 맥락 LLM을 사용하여 더욱 정확하고 유연한 질의응답 시스템을 구축할 수 있다는 점을 강조합니다. \n",
      "3. 논문은 긴 맥락 LLM을 기반으로 한 RAG 모델의 효과를 검증하기 위해 다양한 데이터셋을 사용하여 실험을 수행했습니다. \n",
      "4. 실험 결과, 긴 맥락 LLM을 사용하는 RAG 모델이 기존의 모델보다 뛰어난 성능을 보여주었습니다. \n",
      "5. 또한, 논문은 긴 맥락 LLM을 사용하여 RAG 모델을 효율적으로 학습하는 방법을 제안합니다. \n",
      "6. 마지막으로, 논문은 긴 맥락 LLM 기반 RAG 모델의 미래 연구 방향을 제시합니다. \n",
      "\n",
      "\n",
      "\n",
      "===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 9/11 [00:57<00:20, 10.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# 8\n",
      "본 논문은 Long-Context LLMs를 활용한 RAG(Retrieval Augmented Generation)의 새로운 접근 방식을 제시하며, 특히 중간 단계의 추론을 통한 RAG 성능 향상에 초점을 맞추고 있습니다. \n",
      "\n",
      "첫째, 논문은 다양한 데이터셋(NQ, Wizard of Wikipedia, FEVER, MMLU)에 대한 RAG 학습을 위한 새로운 지시문 템플릿을 제안합니다. 둘째, Gemini-1.5-pro 모델을 사용하여 중간 단계 추론을 생성하도록 지시하는 프롬프트를 설계하고, 이를 통해 RAG 모델의 추론 능력을 향상시키는 방법을 제시합니다. 셋째, 다양한 지시문 템플릿과 프롬프트를 사용하여 실험을 진행하고, 결과를 분석하여 중간 단계 추론이 RAG 성능에 미치는 영향을 규명합니다. 넷째, 데이터 증강 기법을 적용하여 RAG 모델의 성능을 더욱 향상시키는 사례 연구를 제시합니다. 다섯째, 실험 결과를 통해 중간 단계 추론이 RAG 모델의 정확도와 품질을 향상시키는 효과를 입증합니다. 마지막으로, 향후 연구 방향으로 중간 단계 추론을 더욱 발전시키고 다양한 RAG 도메인에 적용할 수 있는 가능성을 제시합니다.\n",
      "\n",
      "\n",
      "\n",
      "===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 10/11 [01:07<00:10, 10.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# 9\n",
      "본 논문은 다양한 규모의 LLMs(Large Language Models)를 사용하여 RAG(Retrieval Augmented Generation) 기법을 적용하여 질문 답변 작업 성능을 향상시키는 방법을 연구합니다. 특히, Gemma-2-9B, Mistral-Nemo-12B와 같은 모델들을 사용하여 데이터 증강된 RAG fine-tuning 기법을 적용하고, 이를 통해 기존의 chat LLM fine-tuning이나 직접 질문-답변 쌍으로 fine-tuning한 방법보다 성능이 향상됨을 보여줍니다. \n",
      "\n",
      "첫째, Gemma-2-9B 모델을 사용하여 다양한 데이터셋에서 RAG fine-tuning의 효과를 분석합니다. RAG fine-tuning은 기존의 chat LLM보다 질문 답변 정확도를 높이는 데 효과적이며, 특히 intermediate reasoning을 추가한 RAG FT w. Int 기법은 더욱 향상된 성능을 보여줍니다. 둘째, Mistral-Nemo-12B 모델을 사용하여 데이터 증강된 RAG fine-tuning 결과를 보여주며, Gemma-2-9B 모델과 마찬가지로 RAG fine-tuning이 성능 향상에 효과적임을 확인합니다. 셋째, 논문은 RAG 기법이 긴 텍스트 입력을 처리하는 데 효과적인 방법임을 보여주며, 이는 LLMs의 긴 텍스트 처리 능력을 향상시키는 데 기여할 수 있습니다. \n",
      "\n",
      "결론적으로, 본 논문은 데이터 증강된 RAG fine-tuning 기법이 LLMs의 질문 답변 성능을 향상시키는 효과적인 방법임을 보여주며, 특히 긴 텍스트 입력 처리에 유용함을 강조합니다.\n",
      "\n",
      "\n",
      "\n",
      "===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [01:21<00:00,  7.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# 10\n",
      "본 논문은 긴 입력에 대한 RAG(Retrieval Augmented Generation)의 성능을 향상시키기 위해 Long-Context LLM(Large Language Model)을 사용하는 방법을 연구합니다. \n",
      "\n",
      "먼저, Mistral-Nemo-12B와 Gemini-1.0-Pro와 같은 다양한 LLM 기반 모델을 사용하여 RAG 특화 튜닝 방법을 평가합니다.  결과는 RAG 특화 튜닝이 LLM의 RAG 성능을 향상시키는 데 효과적임을 보여줍니다. 특히, \"intermediate reasoning step\"을 포함하는 RAG FT w. Int 방법이 가장 우수한 성능을 보입니다. \n",
      "\n",
      "또한, RAG 특화 튜닝 데이터의 크기가 LLM 성능에 미치는 영향을 분석합니다.  결과는 데이터 크기가 증가함에 따라 RAG 성능이 향상됨을 보여주며, 특히 200k개의 데이터를 사용했을 때 가장 높은 성능을 보입니다. \n",
      "\n",
      "마지막으로, RAG 특화 데이터와 일반적인 SFT(Supervised Fine-Tuning) 데이터를 결합하여 LLM을 훈련하는 방법을 제시하고, 이 방법이 LLM의 RAG 성능을 향상시키면서도 일반적인 언어 처리 능력을 유지할 수 있음을 보여줍니다. \n",
      "\n",
      "\n",
      "\n",
      "===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Map 과정 : 각 문서에 대해 요약을 생성합니다.\n",
    "\n",
    "map_prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', '''주어진 논문의 내용을 읽고 한국어로 요약하세요.\n",
    "요약은 1개의 문단과 문단별 6개의 문장으로 작성하세요.\n",
    "답변은 한국어로 작성하세요.\n",
    "같은 내용을 반복하지 마세요.\n",
    "'''),\n",
    "    ('user', '''{text}''')])\n",
    "\n",
    "map_chain  = map_prompt | gemma2 | StrOutputParser()\n",
    "\n",
    "raw_summaries = []\n",
    "for i in tqdm(range(len(document_list))):\n",
    "    response = map_chain.invoke(document_list[i].page_content)\n",
    "    raw_summaries.append(response)\n",
    "    print('')\n",
    "    print('#',i)\n",
    "    print(response)\n",
    "    print('===========================')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['본 논문은 장문 맥락을 처리할 수 있는 대규모 언어 모델(LLM)을 기반으로 한 정보 검색 강화 생성(RAG) 시스템의 새로운 과제와 해결책을 제시합니다. \\n\\n장문 맥락 LLM을 RAG에 적용할 때, 단순히 검색된 문서의 개수를 늘리는 것이 성능 향상을 보장하지 않는다는 것을 발견했습니다. 오히려 검색된 문서의 개수가 증가함에 따라 성능이 감소하는 경우도 있었습니다. 이는 검색된 문서 중에서도 관련성이 낮거나 오히려 오류를 유발할 수 있는 \"hard negatives\"가 LLM의 생성 과정에 부정적인 영향을 미치기 때문입니다. \\n\\n논문에서는 이러한 문제를 해결하기 위해 \"retrieval reordering\", \"implicit robustness fine-tuning\", \"explicit relevance fine-tuning\" 세 가지 방법을 제안합니다. 첫 번째 방법은 검색된 문서의 순서를 재정렬하여 LLM이 더욱 관련성 있는 정보에 집중하도록 유도합니다. 두 번째 방법은 LLM을 노이즈가 포함된 데이터로 fine-tuning하여 hard negatives에 대한 내성을 향상시킵니다. 마지막 방법은 LLM이 검색된 문서를 분석하고 관련 정보를 명시적으로 식별하도록 학습시켜 LLM의 정보 필터링 능력을 강화합니다. \\n\\n이러한 방법들은 장문 맥락 LLM을 기반으로 한 RAG 시스템의 성능을 향상시키는 데 효과적임을 보여줍니다. 또한, 데이터 분포, 사용된 검색 알고리즘, 학습 맥락 길이와 같은 RAG-specific LLM fine-tuning에 영향을 미치는 요소들을 분석하여 실질적인 지침을 제공합니다.\\n\\n\\n',\n",
       " '본 논문은 장문맥 LLM(Long-Context LLMs)을 기반으로 한 RAG(Retrieval-Augmented Generation) 시스템에서 발생하는 과제들을 분석하고 해결 방안을 제시합니다. \\n\\n첫째, 장문맥 LLM이 처리할 수 있는 답변 텍스트의 길이가 증가함에 따라 RAG 성능이 증가하지만, 특정 지점 이후에는 감소하는 경향을 보인다는 것을 발견했습니다. 이는 강력한 검색기(retriever)가 더 많은 관련 텍스트를 검색하지만, 이 중 일부는 실제로는 관련성이 낮거나 오히려 오답을 유발하는 \"hard negatives\"를 포함할 수 있기 때문입니다. 둘째, 높은 정확도를 보이는 검색기가 \"hard negatives\"를 더 많이 검색하여 LLM 성능에 더 큰 부정적인 영향을 미칠 수 있다는 것을 알 수 있습니다. 즉, 단순히 검색 결과의 양보다 검색 결과의 질, 특히 \"hard negatives\"의 양과 특성이 LLM 성능에 큰 영향을 미친다는 것을 시사합니다. \\n\\n이러한 연구 결과를 바탕으로, 장문맥 LLM을 RAG에 적용할 때 \"hard negatives\"를 효과적으로 처리하고 LLM이 검색된 정보를 효율적으로 활용할 수 있도록 하는 새로운 방법론 개발이 필요함을 제시합니다.\\n\\n\\n',\n",
       " '본 논문은 긴 텍스트 컨텍스트에서의 RAG(Retrieval Augmented Generation) 시스템에서 긴 텍스트 컨텍스트를 가진 LLM(Large Language Model)의 성능을 향상시키는 방법을 연구합니다. \\n\\n첫째, 논문은 기존의 평가 지표인 정확도가 긴 텍스트 컨텍스트에서의 LLM 성능을 충분히 반영하지 못한다는 점을 보여줍니다. 특히, \"hard negatives\" (참과 거짓을 구분하기 어려운 관련 없는 텍스트)가 LLM의 성능에 큰 영향을 미치며, 이를 고려한 새로운 평가 방법론이 필요함을 강조합니다. \\n\\n둘째, 논문은 \"retrieval reordering\"이라는 쉬운 방법을 제시합니다. 이 방법은 LLM이 긴 텍스트 컨텍스트에서 \"hard negatives\"의 영향을 덜 받도록, 검색된 텍스트의 순서를 재정렬하는 것입니다. 실험 결과, retrieval reordering은 특히 많은 텍스트가 검색되었을 때 LLM의 성능을 향상시키는 데 효과적임을 보여줍니다.\\n\\n셋째, 논문은 RAG에 특화된 fine-tuning 방법을 통해 LLM의 \"hard negatives\"에 대한 내성을 향상시킬 수 있다고 제안합니다. 이 방법은 LLM을 다양한 컨텍스트에서 학습시켜 \"hard negatives\"를 효과적으로 식별하고 처리할 수 있도록 돕습니다.\\n\\n\\n요약하자면, 본 논문은 긴 텍스트 컨텍스트에서의 RAG 시스템에서 LLM의 성능을 향상시키기 위한 새로운 접근 방식을 제시합니다. 특히, \"hard negatives\"의 문제점을 인식하고 이를 해결하기 위한 retrieval reordering과 RAG-specific fine-tuning 방법을 제안하며, 이를 통해 LLM의 실제 응용 환경에서의 성능을 향상시킬 수 있음을 보여줍니다. \\n',\n",
       " '본 논문은 Long-Context LLM을 RAG(Retrieval Augmented Generation)에 적용하여 성능을 향상시키는 방법을 제시합니다. \\n\\n첫째, 논문은 RAG-specific fine-tuning을 통해 LLM의 Robustness를 향상시키는 방법을 소개합니다. 이 방법은 LLM을 instruction, query, retrieved passages를 입력으로 받아 정답을 생성하도록 학습시키는 방식으로, 다양한 retrieved context를 학습하여 hard negatives에 대한 저항력을 높입니다. 실험 결과, RAG-specific fine-tuning은 기존의 chat model과 Direct fine-tuning보다 모든 데이터셋에서 우수한 성능을 보였습니다.\\n\\n둘째, 논문은 intermediate reasoning을 추가하여 LLM의 relevance identification 능력을 향상시키는 방법을 제시합니다. 이 방법은 LLM이 reasoning paragraph를 생성하여 relevant passages를 명시적으로 식별하도록 학습시키는 방식으로, LLM이 retrieved context에서 중요한 정보를 효과적으로 추출하도록 돕습니다. 실험 결과, intermediate reasoning을 추가한 RAG-specific fine-tuning은 기존의 RAG-specific fine-tuning보다 더 높은 성능을 보였습니다.\\n\\n셋째, 논문은 RAG fine-tuning에 사용되는 데이터 분포, retriever, training context length의 영향을 분석합니다. 다양한 데이터 분포, 여러 retriever를 사용한 fine-tuning, 최대 context length로 fine-tuning하는 것이 LLM의 성능 향상에 효과적임을 확인했습니다.\\n\\n결론적으로, 본 논문은 RAG에 적용된 Long-Context LLM의 성능을 향상시키기 위한 다양한 방법을 제시하고, 실험 결과를 통해 그 효과를 입증합니다.\\n\\n\\n',\n",
       " '본 논문은 장 문맥 LLM이 Retrieval-Augmented Generation(RAG) 시스템에서 장 문맥을 처리하는 데 어려움을 겪는 문제를 해결하기 위해 다양한 접근 방식을 제안하고 평가합니다. \\n\\n첫째, 논문은 여러 개의 검색된 문장을 포함할수록 성능이 향상되지만, 특정한 \"hard negatives\"로 인해 성능이 저하되는 현상을 발견했습니다. 둘째, 이 문제를 해결하기 위해 검색된 문장의 순서를 재정렬하는 방법, RAG에 특화된 암묵적 LLM fine-tuning, 그리고 중간 추론을 포함하는 RAG-oriented LLM fine-tuning을 제안했습니다. 셋째, 데이터 분포, 훈련에 사용되는 검색기, 훈련 문맥 길이 등의 요소가 훈련된 LLM의 성능에 미치는 영향을 분석했습니다. \\n\\n결론적으로, 논문은 장 문맥 LLM을 RAG에 적용할 때 발생하는 어려움을 해결하기 위한 새로운 방법을 제시하고, 이러한 방법들이 RAG 성능을 향상시킬 수 있음을 보여줍니다. \\n\\n특히, 논문은 다음과 같은 핵심 내용을 다룹니다.\\n\\n* 장 문맥 LLM이 RAG에서 장 문맥을 처리하는 데 어려움을 겪는 문제점을 제시합니다.\\n* 여러 개의 검색된 문장을 포함할수록 성능이 향상되지만, \"hard negatives\"로 인해 성능이 저하되는 현상을 발견합니다.\\n* 검색된 문장의 순서 재정렬, RAG-특화 암묵적 fine-tuning, 중간 추론을 포함하는 RAG-oriented fine-tuning을 제안합니다.\\n* 데이터 분포, 검색기, 훈련 문맥 길이 등이 훈련된 LLM의 성능에 미치는 영향을 분석합니다.\\n* 제안된 방법들이 RAG 성능을 향상시킬 수 있음을 보여줍니다.\\n\\n\\n',\n",
       " '본 논문은 장문 맥락을 이해하는 대규모 언어 모델(LLM)이 Retrieval Augmented Generation(RAG)과 같은 기존 질의응답 시스템과 어떻게 상호 작용하는지 연구합니다. \\n\\n특히, 장문 맥락 LLM이 RAG에 적용될 때 발생하는 새로운 과제들을 분석하고, 이러한 과제들을 극복하기 위한 방법들을 제시합니다. \\n\\n첫째, 장문 맥락 LLM이 많은 양의 검색 결과를 처리할 때 오히려 성능이 저하될 수 있다는 것을 발견했습니다. 이는 검색 결과 중에서 관련성이 낮거나 오히려 오답을 제공하는 정보가 LLM의 성능을 방해하기 때문입니다. \\n\\n둘째, 장문 맥락 LLM은 강력한 검색 엔진이 제공하는 검색 결과에 더 취약하다는 것을 알 수 있습니다. 이는 강력한 검색 엔진이 더 많은 관련성이 높은 정보를 찾을 수 있지만, 동시에 더 많은 오답 정보도 제공할 수 있기 때문입니다. \\n\\n셋째, 논문은 장문 맥락 LLM의 성능을 평가할 때 \"hard negative\" (정답이 아닌 관련성이 높은 오답)를 고려해야 함을 강조합니다. \\n\\n마지막으로, 논문은 이러한 문제들을 해결하기 위해 다양한 방법을 제안합니다. 예를 들어, 검색 결과의 품질을 향상시키거나, LLM이 오답 정보를 효과적으로 필터링할 수 있도록 학습시키는 방법이 있습니다. \\n\\n이러한 연구는 장문 맥락 LLM을 RAG에 적용하는 데 있어서 중요한 과제들을 제시하며, 향후 LLM 기반 질의응답 시스템의 발전에 기여할 것으로 기대됩니다.\\n\\n\\n',\n",
       " '본 논문은 긴 텍스트 입력에 대한 Retrieval Augmented Generation(RAG) 시스템의 성능을 향상시키기 위한 방법을 제시합니다. 특히, 긴 맥락을 이해하는 대규모 언어 모델(LLM)을 RAG에 적용했을 때 발생하는 어려움을 해결하는 데 초점을 맞춥니다. \\n\\n첫째, 논문은 LLM이 긴 입력에 대해 효율적으로 학습하고 처리하기 위해 텍스트를 효과적으로 요약하는 방법을 제시합니다. 둘째, 논문은 다양한 Retriever를 사용하여 관련성이 높은 텍스트를 추출하는 방법을 비교 분석하고, LLM의 성능 향상에 가장 효과적인 Retriever를 선택하는 방법을 제안합니다. 셋째, 논문은 LLM이 훈련 데이터에서 학습한 지식을 바탕으로 새로운 질문에 대한 답변을 생성하는 과정에서 발생하는 오류를 줄이기 위한 방법을 제시합니다. 넷째, 논문은 LLM이 긴 텍스트에서 중요한 정보를 추출하고 이해하는 능력을 향상시키기 위한 새로운 기술을 연구합니다. 다섯째, 논문은 LLM을 RAG에 적용했을 때 발생하는 윤리적 문제점을 논의하고, 이러한 문제점을 해결하기 위한 방안을 제시합니다. 마지막으로, 논문은 LLM 기반 RAG 시스템의 미래 전망을 제시하고, 향후 연구 방향을 제시합니다.\\n\\n\\n',\n",
       " '본 논문은 긴 입력에 대한 RAG(Retrieval Augmented Generation)의 과제를 해결하기 위해 긴 맥락 LLM(Long-Context Large Language Model)을 사용하는 방법을 제시합니다. \\n\\n1. 논문은 긴 입력을 처리하는 RAG 모델을 개발하기 위한 다양한 접근 방식을 소개합니다. \\n2. 특히, 긴 맥락 LLM을 사용하여 더욱 정확하고 유연한 질의응답 시스템을 구축할 수 있다는 점을 강조합니다. \\n3. 논문은 긴 맥락 LLM을 기반으로 한 RAG 모델의 효과를 검증하기 위해 다양한 데이터셋을 사용하여 실험을 수행했습니다. \\n4. 실험 결과, 긴 맥락 LLM을 사용하는 RAG 모델이 기존의 모델보다 뛰어난 성능을 보여주었습니다. \\n5. 또한, 논문은 긴 맥락 LLM을 사용하여 RAG 모델을 효율적으로 학습하는 방법을 제안합니다. \\n6. 마지막으로, 논문은 긴 맥락 LLM 기반 RAG 모델의 미래 연구 방향을 제시합니다. \\n\\n\\n',\n",
       " '본 논문은 Long-Context LLMs를 활용한 RAG(Retrieval Augmented Generation)의 새로운 접근 방식을 제시하며, 특히 중간 단계의 추론을 통한 RAG 성능 향상에 초점을 맞추고 있습니다. \\n\\n첫째, 논문은 다양한 데이터셋(NQ, Wizard of Wikipedia, FEVER, MMLU)에 대한 RAG 학습을 위한 새로운 지시문 템플릿을 제안합니다. 둘째, Gemini-1.5-pro 모델을 사용하여 중간 단계 추론을 생성하도록 지시하는 프롬프트를 설계하고, 이를 통해 RAG 모델의 추론 능력을 향상시키는 방법을 제시합니다. 셋째, 다양한 지시문 템플릿과 프롬프트를 사용하여 실험을 진행하고, 결과를 분석하여 중간 단계 추론이 RAG 성능에 미치는 영향을 규명합니다. 넷째, 데이터 증강 기법을 적용하여 RAG 모델의 성능을 더욱 향상시키는 사례 연구를 제시합니다. 다섯째, 실험 결과를 통해 중간 단계 추론이 RAG 모델의 정확도와 품질을 향상시키는 효과를 입증합니다. 마지막으로, 향후 연구 방향으로 중간 단계 추론을 더욱 발전시키고 다양한 RAG 도메인에 적용할 수 있는 가능성을 제시합니다.\\n\\n\\n',\n",
       " '본 논문은 다양한 규모의 LLMs(Large Language Models)를 사용하여 RAG(Retrieval Augmented Generation) 기법을 적용하여 질문 답변 작업 성능을 향상시키는 방법을 연구합니다. 특히, Gemma-2-9B, Mistral-Nemo-12B와 같은 모델들을 사용하여 데이터 증강된 RAG fine-tuning 기법을 적용하고, 이를 통해 기존의 chat LLM fine-tuning이나 직접 질문-답변 쌍으로 fine-tuning한 방법보다 성능이 향상됨을 보여줍니다. \\n\\n첫째, Gemma-2-9B 모델을 사용하여 다양한 데이터셋에서 RAG fine-tuning의 효과를 분석합니다. RAG fine-tuning은 기존의 chat LLM보다 질문 답변 정확도를 높이는 데 효과적이며, 특히 intermediate reasoning을 추가한 RAG FT w. Int 기법은 더욱 향상된 성능을 보여줍니다. 둘째, Mistral-Nemo-12B 모델을 사용하여 데이터 증강된 RAG fine-tuning 결과를 보여주며, Gemma-2-9B 모델과 마찬가지로 RAG fine-tuning이 성능 향상에 효과적임을 확인합니다. 셋째, 논문은 RAG 기법이 긴 텍스트 입력을 처리하는 데 효과적인 방법임을 보여주며, 이는 LLMs의 긴 텍스트 처리 능력을 향상시키는 데 기여할 수 있습니다. \\n\\n결론적으로, 본 논문은 데이터 증강된 RAG fine-tuning 기법이 LLMs의 질문 답변 성능을 향상시키는 효과적인 방법임을 보여주며, 특히 긴 텍스트 입력 처리에 유용함을 강조합니다.\\n\\n\\n',\n",
       " '본 논문은 긴 입력에 대한 RAG(Retrieval Augmented Generation)의 성능을 향상시키기 위해 Long-Context LLM(Large Language Model)을 사용하는 방법을 연구합니다. \\n\\n먼저, Mistral-Nemo-12B와 Gemini-1.0-Pro와 같은 다양한 LLM 기반 모델을 사용하여 RAG 특화 튜닝 방법을 평가합니다.  결과는 RAG 특화 튜닝이 LLM의 RAG 성능을 향상시키는 데 효과적임을 보여줍니다. 특히, \"intermediate reasoning step\"을 포함하는 RAG FT w. Int 방법이 가장 우수한 성능을 보입니다. \\n\\n또한, RAG 특화 튜닝 데이터의 크기가 LLM 성능에 미치는 영향을 분석합니다.  결과는 데이터 크기가 증가함에 따라 RAG 성능이 향상됨을 보여주며, 특히 200k개의 데이터를 사용했을 때 가장 높은 성능을 보입니다. \\n\\n마지막으로, RAG 특화 데이터와 일반적인 SFT(Supervised Fine-Tuning) 데이터를 결합하여 LLM을 훈련하는 방법을 제시하고, 이 방법이 LLM의 RAG 성능을 향상시키면서도 일반적인 언어 처리 능력을 유지할 수 있음을 보여줍니다. \\n\\n\\n']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ckS-TZVsMpoa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 장문 맥락 LLM 기반 RAG 시스템 성능 향상 방안 연구: \"hard negatives\" 문제 해결과 효과적인 fine-tuning 방법 제시\n",
      "\n",
      "본 논문은 장문 맥락을 이해하는 대규모 언어 모델(LLM)을 Retrieval-Augmented Generation(RAG) 시스템에 적용할 때 발생하는 새로운 과제들을 분석하고 해결책을 제시합니다. \n",
      "\n",
      "**핵심 내용:**\n",
      "\n",
      "* **장문 맥락 LLM의 RAG 적용 시 \"hard negatives\" 문제:** 단순히 검색된 문서의 개수를 늘리는 것이 성능 향상을 보장하지 않는다는 것을 발견했습니다. 오히려 검색된 문서 중 관련성이 낮거나 오히려 오류를 유발하는 \"hard negatives\"가 LLM의 생성 과정에 부정적인 영향을 미치는 것을 확인했습니다.\n",
      "* **\"hard negatives\" 문제 해결을 위한 세 가지 방법 제안:**\n",
      "    * **retrieval reordering:** 검색된 문서의 순서를 재정렬하여 LLM이 더욱 관련성 있는 정보에 집중하도록 유도합니다.\n",
      "    * **implicit robustness fine-tuning:** LLM을 노이즈가 포함된 데이터로 fine-tuning하여 hard negatives에 대한 내성을 향상시킵니다.\n",
      "    * **explicit relevance fine-tuning:** LLM이 검색된 문서를 분석하고 관련 정보를 명시적으로 식별하도록 학습시켜 정보 필터링 능력을 강화합니다.\n",
      "* **RAG-specific fine-tuning의 중요성:** 데이터 분포, 사용된 검색 알고리즘, 학습 맥락 길이와 같은 요소들이 RAG 시스템의 성능에 영향을 미친다는 것을 분석하고 실질적인 지침을 제공합니다.\n",
      "\n",
      "**결론:**\n",
      "\n",
      "본 논문은 장문 맥락 LLM을 기반으로 한 RAG 시스템의 성능을 향상시키는 데 효과적인 방법을 제시합니다. 특히, \"hard negatives\" 문제를 인식하고 이를 해결하기 위한 retrieval reordering, RAG-specific fine-tuning 방법을 제안하며, 이를 통해 LLM의 실제 응용 환경에서의 성능을 향상시킬 수 있음을 보여줍니다. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reduce 과정 : 각 문서의 요약을 하나로 합칩니다.\n",
    "reduce_prompt = ChatPromptTemplate.from_messages([\n",
    "\n",
    "    ('system', '''\n",
    "Generate a summary of the following text that includes the following elements:\n",
    "\n",
    "* A title that accurately reflects the content of the text.\n",
    "* An introduction paragraph that provides an overview of the topic.\n",
    "* Bullet points that list the key points of the text.\n",
    "* A conclusion paragraph that summarizes the main points of the text.\n",
    "\n",
    "Answer in Korean.\n",
    "'''),\n",
    "    ('user', '''{text}\n",
    "---\n",
    "요약(In Korean):\n",
    "''')])\n",
    "\n",
    "reduce_chain = reduce_prompt | gemma2 | StrOutputParser()\n",
    "\n",
    "summary = reduce_chain.invoke('\\n---\\n'.join(raw_summaries))\n",
    "\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pi84F2cYMpoa"
   },
   "source": [
    "## Refine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ZfANrVtMpoa"
   },
   "source": [
    "Refine은 청크를 순서대로 참고하며, 매 시점 요약문을 작성합니다.   \n",
    "요약문과 새로운 청크를 비교하여, 요약문을 업데이트합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "7dee-KnHMpob"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "본 논문은 장문 맥락을 가진 대규모 언어 모델(LLM)을 사용하는 Retrieval-Augmented Generation(RAG) 시스템의 새로운 과제와 해결책을 제시합니다.  장문 맥락 LLM은 더 많은 정보를 활용할 수 있지만, 이는 오히려 성능 저하로 이어질 수 있습니다. 이는 LLM이 많은 정보 중에서도 관련성이 낮은 정보(hard negatives)에 영향을 받기 때문입니다.  \n",
      "\n",
      "논문은 장문 맥락 LLM을 사용하는 RAG 시스템에서 hard negatives가 성능에 미치는 부정적인 영향을 분석합니다. 단순히 더 많은 정보를 제공하는 것이 항상 좋은 결과를 가져오지 않는다는 것을 보여주며, 강력한 검색기(retriever)를 사용하는 것조차 문제를 악화시킬 수 있다는 것을 발견했습니다. 이러한 문제를 해결하기 위해 논문은 다음과 같은 세 가지 방법을 제안합니다. 첫째, 검색된 문서의 순서를 재정렬하여 LLM의 주의를 더 관련성 있는 정보로 유도합니다. 둘째, LLM을 잠재적인 노이즈를 포함한 쿼리와 검색된 문서 데이터로 fine-tuning하여 hard negatives에 대한 내성을 향상시킵니다. 셋째, LLM을 중간 단계의 추론을 통해 검색된 문서를 분석하고 관련 정보를 명시적으로 식별하도록 학습하여 LLM의 관련성 파악 능력을 향상시킵니다. \n",
      "\n",
      "이러한 방법들은 데이터 분포, 사용된 검색기, 학습 맥락 길이와 같은 다양한 요소에 대한 광범위한 연구를 통해 개발되었습니다. 논문은 장문 맥락 LLM을 사용하는 RAG 시스템의 성능을 향상시키는 데 기여하며, 이러한 시스템의 잠재력을 더욱 탐구할 수 있는 방향을 제시합니다. \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "first_summarize_prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', '''당신은 인공지능 전문가입니다.\n",
    "주어진 논문의 내용을 읽고 한국어로 요약하세요.\n",
    "요약은 1개의 문단과 문단별 5개의 문장으로 작성하세요.\n",
    "답변은 한국어로 작성하세요.'''),\n",
    "    ('user', '''{text}''')])\n",
    "\n",
    "\n",
    "\n",
    "X = first_summarize_prompt.format_messages(text=document_list[0])\n",
    "\n",
    "intermediate_summary = gemma2.invoke(X).content\n",
    "print(intermediate_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "bBpA1i76Mpob"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:02<00:20,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "본 논문은 장문 맥락을 이해하는 대규모 언어 모델(LLM)을 사용하는 Retrieval-Augmented Generation(RAG) 시스템의 새로운 과제와 해결책을 제시합니다. 장문 맥락 LLM은 더 많은 정보를 활용할 수 있지만, 이는 오히려 성능 저하로 이어질 수 있습니다. 이는 LLM이 많은 정보 중에서도 관련성이 낮은 정보(hard negatives)에 영향을 받기 때문입니다. \n",
      "\n",
      "논문은 장문 맥락 LLM을 사용하는 RAG 시스템에서 hard negatives가 성능에 미치는 부정적인 영향을 분석합니다. 단순히 더 많은 정보를 제공하는 것이 항상 좋은 결과를 가져오지 않는다는 것을 보여주며, 강력한 검색기(retriever)를 사용하는 것조차 문제를 악화시킬 수 있다는 것을 발견했습니다. 이러한 문제를 해결하기 위해 논문은 다음과 같은 세 가지 방법을 제안합니다. 첫째, 검색된 문서의 순서를 재정렬하여 LLM의 주의를 더 관련성 있는 정보로 유도합니다. 둘째, LLM을 잠재적인 노이즈를 포함한 쿼리와 검색된 문서 데이터로 fine-tuning하여 hard negatives에 대한 내성을 향상시킵니다. 셋째, LLM을 중간 단계의 추론을 통해 검색된 문서를 분석하고 관련 정보를 명시적으로 식별하도록 학습하여 LLM의 관련성 파악 능력을 향상시킵니다. \n",
      "\n",
      "이러한 방법들은 데이터 분포, 사용된 검색기, 학습 맥락 길이와 같은 다양한 요소에 대한 광범위한 연구를 통해 개발되었습니다. 논문은 장문 맥락 LLM을 사용하는 RAG 시스템의 성능을 향상시키는 데 기여하며, 이러한 시스템의 잠재력을 더욱 탐구할 수 있는 방향을 제시합니다. 특히, 강력한 검색기가 더 많은 관련 정보를 제공하지만, 이로 인해 발생하는 hard negatives 문제를 해결하기 위한 방법을 제시합니다. \n",
      "\n",
      "\n",
      "\n",
      "=======================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:03<00:13,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "본 논문은 장문 맥락을 이해하는 대규모 언어 모델(LLM)을 사용하는 Retrieval-Augmented Generation(RAG) 시스템에서 발생하는 새로운 과제와 해결책을 제시합니다. 장문 맥락 LLM은 더 많은 정보를 활용할 수 있지만, 이는 오히려 성능 저하로 이어질 수 있습니다. 이는 LLM이 많은 정보 중에서도 관련성이 낮은 정보(hard negatives)에 영향을 받기 때문입니다. \n",
      "\n",
      "논문은 장문 맥락 LLM을 사용하는 RAG 시스템에서 hard negatives가 성능에 미치는 부정적인 영향을 분석합니다. 단순히 더 많은 정보를 제공하는 것이 항상 좋은 결과를 가져오지 않는다는 것을 보여주며, 강력한 검색기(retriever)를 사용하는 것조차 문제를 악화시킬 수 있다는 것을 발견했습니다. 이러한 문제를 해결하기 위해 논문은 다음과 같은 세 가지 방법을 제안합니다. 첫째, 검색된 문서의 순서를 재정렬하여 LLM의 주의를 더 관련성 있는 정보로 유도합니다. 둘째, LLM을 잠재적인 노이즈를 포함한 쿼리와 검색된 문서 데이터로 fine-tuning하여 hard negatives에 대한 내성을 향상시킵니다. 셋째, LLM을 중간 단계의 추론을 통해 검색된 문서를 분석하고 관련 정보를 명시적으로 식별하도록 학습하여 LLM의 관련성 파악 능력을 향상시킵니다. \n",
      "\n",
      "특히, 강력한 검색기가 더 많은 관련 정보를 제공하지만, 이로 인해 발생하는 hard negatives 문제를 해결하기 위한 방법을 제시합니다.  논문은 검색된 문서의 순서를 재정렬하는 방법을 통해 LLM이 hard negatives에 영향을 받지 않도록 유도하고, LLM을 쿼리와 검색된 문서 데이터로 fine-tuning하여 hard negatives에 대한 내성을 높이는 방법을 제안합니다. 또한, LLM이 검색된 문서를 분석하고 관련 정보를 명시적으로 식별하도록 학습하는 방법을 통해 LLM의 관련성 파악 능력을 향상시키는 방법을 제시합니다. \n",
      "\n",
      "\n",
      "\n",
      "=======================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:05<00:11,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "본 논문은 장문 맥락을 이해하는 대규모 언어 모델(LLM)을 사용하는 Retrieval-Augmented Generation(RAG) 시스템에서 발생하는 새로운 과제와 해결책을 제시합니다. 장문 맥락 LLM은 더 많은 정보를 활용할 수 있지만, 이는 오히려 성능 저하로 이어질 수 있습니다. 이는 LLM이 많은 정보 중에서도 관련성이 낮은 정보(hard negatives)에 영향을 받기 때문입니다. \n",
      "\n",
      "논문은 장문 맥락 LLM을 사용하는 RAG 시스템에서 hard negatives가 성능에 미치는 부정적인 영향을 분석합니다. 단순히 더 많은 정보를 제공하는 것이 항상 좋은 결과를 가져오지 않는다는 것을 보여주며, 강력한 검색기(retriever)를 사용하는 것조차 문제를 악화시킬 수 있다는 것을 발견했습니다. 이러한 문제를 해결하기 위해 논문은 다음과 같은 세 가지 방법을 제안합니다. 첫째, 검색된 문서의 순서를 재정렬하여 LLM의 주의를 더 관련성 있는 정보로 유도합니다. 둘째, LLM을 잠재적인 노이즈를 포함한 쿼리와 검색된 문서 데이터로 fine-tuning하여 hard negatives에 대한 내성을 향상시킵니다. 셋째, LLM을 중간 단계의 추론을 통해 검색된 문서를 분석하고 관련 정보를 명시적으로 식별하도록 학습하여 LLM의 관련성 파악 능력을 향상시킵니다.\n",
      "\n",
      "특히, 강력한 검색기가 더 많은 관련 정보를 제공하지만, 이로 인해 발생하는 hard negatives 문제를 해결하기 위한 방법을 제시합니다. 논문은 검색된 문서의 순서를 재정렬하는 방법을 통해 LLM이 hard negatives에 영향을 받지 않도록 유도하고, LLM을 쿼리와 검색된 문서 데이터로 fine-tuning하여 hard negatives에 대한 내성을 높이는 방법을 제안합니다. 또한, LLM이 검색된 문서를 분석하고 관련 정보를 명시적으로 식별하도록 학습하는 방법을 통해 LLM의 관련성 파악 능력을 향상시키는 방법을 제시합니다. \n",
      "\n",
      "논문은 RAG-specific fine-tuning을 통해 LLM의 성능을 향상시킬 수 있다는 것을 보여주며, 특히 hard negatives 문제를 해결하는 데 효과적임을 보여줍니다. \n",
      "\n",
      "\n",
      "\n",
      "=======================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:07<00:11,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "본 논문은 장문 맥락을 이해하는 대규모 언어 모델(LLM)을 사용하는 Retrieval-Augmented Generation(RAG) 시스템에서 발생하는 새로운 과제와 해결책을 제시합니다. 장문 맥락 LLM은 더 많은 정보를 활용할 수 있지만, 이는 오히려 성능 저하로 이어질 수 있습니다. 이는 LLM이 많은 정보 중에서도 관련성이 낮은 정보(hard negatives)에 영향을 받기 때문입니다. \n",
      "\n",
      "논문은 장문 맥락 LLM을 사용하는 RAG 시스템에서 hard negatives가 성능에 미치는 부정적인 영향을 분석합니다. 단순히 더 많은 정보를 제공하는 것이 항상 좋은 결과를 가져오지 않는다는 것을 보여주며, 강력한 검색기(retriever)를 사용하는 것조차 문제를 악화시킬 수 있다는 것을 발견했습니다. 이러한 문제를 해결하기 위해 논문은 다음과 같은 세 가지 방법을 제안합니다. 첫째, 검색된 문서의 순서를 재정렬하여 LLM의 주의를 더 관련성 있는 정보로 유도합니다. 둘째, LLM을 잠재적인 노이즈를 포함한 쿼리와 검색된 문서 데이터로 fine-tuning하여 hard negatives에 대한 내성을 향상시킵니다. 셋째, LLM을 중간 단계의 추론을 통해 검색된 문서를 분석하고 관련 정보를 명시적으로 식별하도록 학습하여 LLM의 관련성 파악 능력을 향상시킵니다.\n",
      "\n",
      "특히, 강력한 검색기가 더 많은 관련 정보를 제공하지만, 이로 인해 발생하는 hard negatives 문제를 해결하기 위한 방법을 제시합니다. 논문은 검색된 문서의 순서를 재정렬하는 방법을 통해 LLM이 hard negatives에 영향을 받지 않도록 유도하고, LLM을 쿼리와 검색된 문서 데이터로 fine-tuning하여 hard negatives에 대한 내성을 높이는 방법을 제안합니다. 또한, LLM이 검색된 문서를 분석하고 관련 정보를 명시적으로 식별하도록 학습하는 방법을 통해 LLM의 관련성 파악 능력을 향상시키는 방법을 제시합니다. \n",
      "\n",
      "**추가적으로, 논문은 다양한 데이터 분포, 검색기, 맥락 길이 조합을 통해 fine-tuning하는 것이 RAG 성능 향상에 효과적임을 보여줍니다.** 특히, 최대 맥락 길이로 fine-tuning하는 것이 다양한 검색 규모에서 가장 좋은 성능을 보였다는 점이 중요합니다. \n",
      "\n",
      "\n",
      "\n",
      "=======================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:22<00:32,  6.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "본 논문은 장문 맥락을 이해하는 대규모 언어 모델(LLM)을 사용하는 Retrieval-Augmented Generation(RAG) 시스템에서 발생하는 새로운 과제와 해결책을 제시합니다. 장문 맥락 LLM은 더 많은 정보를 활용할 수 있지만, 이는 오히려 성능 저하로 이어질 수 있습니다. 이는 LLM이 많은 정보 중에서도 관련성이 낮은 정보(hard negatives)에 영향을 받기 때문입니다. \n",
      "\n",
      "논문은 장문 맥락 LLM을 사용하는 RAG 시스템에서 hard negatives가 성능에 미치는 부정적인 영향을 분석합니다. 단순히 더 많은 정보를 제공하는 것이 항상 좋은 결과를 가져오지 않는다는 것을 보여주며, 강력한 검색기(retriever)를 사용하는 것조차 문제를 악화시킬 수 있다는 것을 발견했습니다. 이러한 문제를 해결하기 위해 논문은 다음과 같은 세 가지 방법을 제안합니다. 첫째, 검색된 문서의 순서를 재정렬하여 LLM의 주의를 더 관련성 있는 정보로 유도합니다. 둘째, LLM을 잠재적인 노이즈를 포함한 쿼리와 검색된 문서 데이터로 fine-tuning하여 hard negatives에 대한 내성을 향상시킵니다. 셋째, LLM을 중간 단계의 추론을 통해 검색된 문서를 분석하고 관련 정보를 명시적으로 식별하도록 학습하여 LLM의 관련성 파악 능력을 향상시킵니다.\n",
      "\n",
      "특히, 강력한 검색기가 더 많은 관련 정보를 제공하지만, 이로 인해 발생하는 hard negatives 문제를 해결하기 위한 방법을 제시합니다. 논문은 검색된 문서의 순서를 재정렬하는 방법을 통해 LLM이 hard negatives에 영향을 받지 않도록 유도하고, LLM을 쿼리와 검색된 문서 데이터로 fine-tuning하여 hard negatives에 대한 내성을 높이는 방법을 제안합니다. 또한, LLM이 검색된 문서를 분석하고 관련 정보를 명시적으로 식별하도록 학습하는 방법을 통해 LLM의 관련성 파악 능력을 향상시키는 방법을 제시합니다. \n",
      "\n",
      "**추가적으로, 논문은 다양한 데이터 분포, 검색기, 맥락 길이 조합을 통해 fine-tuning하는 것이 RAG 성능 향상에 효과적임을 보여줍니다.** 특히, 최대 맥락 길이로 fine-tuning하는 것이 다양한 검색 규모에서 가장 좋은 성능을 보였다는 점이 중요합니다.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "=======================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:40<00:42, 10.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "본 논문은 장문 맥락을 이해하는 대규모 언어 모델(LLM)을 사용하는 Retrieval-Augmented Generation(RAG) 시스템에서 발생하는 새로운 과제와 해결책을 제시합니다. 장문 맥락 LLM은 더 많은 정보를 활용할 수 있지만, 이는 오히려 성능 저하로 이어질 수 있습니다. 이는 LLM이 많은 정보 중에서도 관련성이 낮은 정보(hard negatives)에 영향을 받기 때문입니다. \n",
      "\n",
      "논문은 강력한 검색기(retriever)를 사용하더라도 hard negatives 문제가 발생하며, 이는 LLM의 성능을 저하시킬 수 있다는 것을 보여줍니다.  논문은 이러한 문제를 해결하기 위해 다음과 같은 세 가지 방법을 제안합니다. 첫째, 검색된 문서의 순서를 재정렬하여 LLM의 주의를 더 관련성 있는 정보로 유도합니다. 둘째, LLM을 잠재적인 노이즈를 포함한 쿼리와 검색된 문서 데이터로 fine-tuning하여 hard negatives에 대한 내성을 향상시킵니다. 셋째, LLM을 중간 단계의 추론을 통해 검색된 문서를 분석하고 관련 정보를 명시적으로 식별하도록 학습하여 LLM의 관련성 파악 능력을 향상시킵니다. \n",
      "\n",
      "특히,  **weaker retrievers (e.g., BM25) 또는 무작위 샘플링**과 같은 방법을 통해 hard negatives를 줄일 수 있다는 것을 보여줍니다. 또한,  **hard negatives는 LLM의 성능에 부정적인 영향을 미치는 주요 요인**이며, 이를 해결하기 위한 노력이 필요함을 강조합니다. \n",
      "\n",
      "\n",
      "\n",
      "=======================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [00:53<00:34, 11.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "본 논문은 장문 맥락을 이해하는 대규모 언어 모델(LLM)을 사용하는 Retrieval-Augmented Generation(RAG) 시스템에서 발생하는 새로운 과제와 해결책을 제시합니다.  장문 맥락 LLM은 더 많은 정보를 활용할 수 있지만, 이는 오히려 성능 저하로 이어질 수 있습니다. 이는 LLM이 많은 정보 중에서도 관련성이 낮은 정보(hard negatives)에 영향을 받기 때문입니다. \n",
      "\n",
      "논문은 강력한 검색기(retriever)를 사용하더라도 hard negatives 문제가 발생하며, 이는 LLM의 성능을 저하시킬 수 있다는 것을 보여줍니다.  논문은 이러한 문제를 해결하기 위해 다음과 같은 세 가지 방법을 제안합니다. 첫째, 검색된 문서의 순서를 재정렬하여 LLM의 주의를 더 관련성 있는 정보로 유도합니다. 둘째, LLM을 잠재적인 노이즈를 포함한 쿼리와 검색된 문서 데이터로 fine-tuning하여 hard negatives에 대한 내성을 향상시킵니다. 셋째, LLM을 중간 단계의 추론을 통해 검색된 문서를 분석하고 관련 정보를 명시적으로 식별하도록 학습하여 LLM의 관련성 파악 능력을 향상시킵니다. \n",
      "\n",
      "특히, **weaker retrievers (e.g., BM25) 또는 무작위 샘플링**과 같은 방법을 통해 hard negatives를 줄일 수 있다는 것을 보여줍니다. 또한,  **hard negatives는 LLM의 성능에 부정적인 영향을 미치는 주요 요인**이며, 이를 해결하기 위한 노력이 필요함을 강조합니다. \n",
      "\n",
      "본 논문은  **Implicit RAG Fine-Tuning**과 **RAG Finetuning with Intermediate Reasoning** 두 가지 새로운 방법을 제시합니다.  Implicit RAG Fine-Tuning은 LLM을 쿼리와 검색된 문서를 함께 입력으로 사용하여 fine-tuning하는 방법이며, RAG Finetuning with Intermediate Reasoning은 LLM이 검색된 문서를 분석하고 추론을 통해 관련 정보를 명시적으로 식별하도록 학습하는 방법입니다. \n",
      "\n",
      "\n",
      "\n",
      "=======================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [01:09<00:25, 12.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "본 논문은 장문 맥락을 이해하는 대규모 언어 모델(LLM)을 사용하는 Retrieval-Augmented Generation(RAG) 시스템에서 발생하는 hard negatives 문제를 해결하기 위한 방법을 제시합니다. \n",
      "\n",
      "hard negatives는 LLM이 관련성이 낮은 정보에 영향을 받아 성능 저하를 초래할 수 있습니다. 논문은 강력한 검색기(retriever)를 사용하더라도 hard negatives 문제가 발생하며, 이는 LLM의 성능을 저하시킬 수 있다는 것을 보여줍니다.  \n",
      "\n",
      "논문은 이러한 문제를 해결하기 위해 세 가지 방법을 제안합니다. 첫째, 검색된 문서의 순서를 재정렬하여 LLM의 주의를 더 관련성 있는 정보로 유도합니다. 둘째, LLM을 잠재적인 노이즈를 포함한 쿼리와 검색된 문서 데이터로 fine-tuning하여 hard negatives에 대한 내성을 향상시킵니다. 셋째, LLM을 중간 단계의 추론을 통해 검색된 문서를 분석하고 관련 정보를 명시적으로 식별하도록 학습하여 LLM의 관련성 파악 능력을 향상시킵니다. \n",
      "\n",
      "특히, weaker retrievers 또는 무작위 샘플링과 같은 방법을 통해 hard negatives를 줄일 수 있다는 것을 보여줍니다. 또한, hard negatives가 LLM의 성능에 부정적인 영향을 미치는 주요 요인이며, 이를 해결하기 위한 노력이 필요함을 강조합니다. \n",
      "\n",
      "본 논문은 Implicit RAG Fine-Tuning과 RAG Finetuning with Intermediate Reasoning 두 가지 새로운 방법을 제시합니다. Implicit RAG Fine-Tuning은 LLM을 쿼리와 검색된 문서를 함께 입력으로 사용하여 fine-tuning하는 방법이며, RAG Finetuning with Intermediate Reasoning은 LLM이 검색된 문서를 분석하고 추론을 통해 관련 정보를 명시적으로 식별하도록 학습하는 방법입니다. \n",
      "\n",
      "\n",
      "\n",
      "=======================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [01:21<00:12, 12.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "본 논문은 장문 맥락을 이해하는 대규모 언어 모델(LLM)을 사용하는 Retrieval-Augmented Generation(RAG) 시스템에서 발생하는 hard negatives 문제를 해결하기 위한 방법을 제시합니다. \n",
      "\n",
      "hard negatives는 LLM이 관련성이 낮은 정보에 영향을 받아 성능 저하를 초래할 수 있습니다. 논문은 강력한 검색기(retriever)를 사용하더라도 hard negatives 문제가 발생하며, 이는 LLM의 성능을 저하시킬 수 있다는 것을 보여줍니다. \n",
      "\n",
      "논문은 이러한 문제를 해결하기 위해 세 가지 방법을 제안합니다. 첫째, 검색된 문서의 순서를 재정렬하여 LLM의 주의를 더 관련성 있는 정보로 유도합니다. 둘째, LLM을 잠재적인 노이즈를 포함한 쿼리와 검색된 문서 데이터로 fine-tuning하여 hard negatives에 대한 내성을 향상시킵니다. 셋째, LLM을 중간 단계의 추론을 통해 검색된 문서를 분석하고 관련 정보를 명시적으로 식별하도록 학습하여 LLM의 관련성 파악 능력을 향상시킵니다. \n",
      "\n",
      "특히, weaker retrievers 또는 무작위 샘플링과 같은 방법을 통해 hard negatives를 줄일 수 있다는 것을 보여줍니다. 또한, hard negatives가 LLM의 성능에 부정적인 영향을 미치는 주요 요인이며, 이를 해결하기 위한 노력이 필요함을 강조합니다. \n",
      "\n",
      "본 논문은 Implicit RAG Fine-Tuning과 RAG Finetuning with Intermediate Reasoning 두 가지 새로운 방법을 제시합니다. Implicit RAG Fine-Tuning은 LLM을 쿼리와 검색된 문서를 함께 입력으로 사용하여 fine-tuning하는 방법이며, RAG Finetuning with Intermediate Reasoning은 LLM이 검색된 문서를 분석하고 추론을 통해 관련 정보를 명시적으로 식별하도록 학습하는 방법입니다. \n",
      "\n",
      "**추가적으로, 논문은 Gemma-2-9B와 Mistral-Nemo-12B와 같은 다양한 LLM 모델에서 데이터 증강 RAG fine-tuning 방법의 효과를 보여줍니다. 특히, RAG Finetuning with Intermediate Reasoning이 Implicit RAG Fine-Tuning보다 더 높은 성능을 보이는 것을 확인할 수 있습니다.**\n",
      "\n",
      "\n",
      "\n",
      "=======================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:38<00:00,  9.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "본 논문은 장문 맥락 이해 능력을 갖춘 대규모 언어 모델(LLM)을 사용하는 Retrieval-Augmented Generation(RAG) 시스템에서 발생하는 hard negatives 문제를 해결하기 위한 방법을 제시합니다. \n",
      "\n",
      "hard negatives는 LLM이 관련성이 낮은 정보에 영향을 받아 성능 저하를 초래할 수 있습니다. 논문은 강력한 검색기(retriever)를 사용하더라도 hard negatives 문제가 발생하며, 이는 LLM의 성능을 저하시킬 수 있다는 것을 보여줍니다. \n",
      "\n",
      "논문은 이러한 문제를 해결하기 위해 세 가지 방법을 제안합니다. 첫째, 검색된 문서의 순서를 재정렬하여 LLM의 주의를 더 관련성 있는 정보로 유도합니다. 둘째, LLM을 잠재적인 노이즈를 포함한 쿼리와 검색된 문서 데이터로 fine-tuning하여 hard negatives에 대한 내성을 향상시킵니다. 셋째, LLM을 중간 단계의 추론을 통해 검색된 문서를 분석하고 관련 정보를 명시적으로 식별하도록 학습하여 LLM의 관련성 파악 능력을 향상시킵니다. \n",
      "\n",
      "특히, weaker retrievers 또는 무작위 샘플링과 같은 방법을 통해 hard negatives를 줄일 수 있다는 것을 보여줍니다. 또한, hard negatives가 LLM의 성능에 부정적인 영향을 미치는 주요 요인이며, 이를 해결하기 위한 노력이 필요함을 강조합니다. \n",
      "\n",
      "본 논문은 Implicit RAG Fine-Tuning과 RAG Finetuning with Intermediate Reasoning 두 가지 새로운 방법을 제시합니다. Implicit RAG Fine-Tuning은 LLM을 쿼리와 검색된 문서를 함께 입력으로 사용하여 fine-tuning하는 방법이며, RAG Finetuning with Intermediate Reasoning은 LLM이 검색된 문서를 분석하고 추론을 통해 관련 정보를 명시적으로 식별하도록 학습하는 방법입니다. \n",
      "\n",
      "**추가적으로, 논문은 Gemma-2-9B와 Mistral-Nemo-12B와 같은 다양한 LLM 모델에서 데이터 증강 RAG fine-tuning 방법의 효과를 보여줍니다. 특히, RAG Finetuning with Intermediate Reasoning이 Implicit RAG Fine-Tuning보다 더 높은 성능을 보이는 것을 확인할 수 있습니다.** 또한, RAG-specific fine-tuning data를 일반적인 SFT data와 결합하면 LLM의 RAG 성능을 향상시키면서도 일반적인 언어 처리 능력을 유지할 수 있다는 것을 보여줍니다.\n",
      "\n",
      "\n",
      "\n",
      "=======================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Refine Prompt\n",
    "\n",
    "refine_prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', '''당신은 인공지능 전문가입니다.\n",
    "논문의 현재 시점까지의 한국어 요약이 주어집니다.\n",
    "이를 읽고, 새롭게 주어지는 내용과 비교하여 한국어 요약을 보완하거나 수정하세요.\n",
    "전체 요약은 10문장 이내로 작성하세요.\n",
    "'''),\n",
    "    ('user', '''현재 시점까지의 요약: {previous_summary}\n",
    "---\n",
    "새로운 내용: {new_text}''')])\n",
    "\n",
    "\n",
    "refine_chain = refine_prompt | gemma2 | StrOutputParser()\n",
    "\n",
    "for i in tqdm(range(1, len(document_list))):\n",
    "    intermediate_summary = refine_chain.invoke(\n",
    "        {'previous_summary':intermediate_summary,\n",
    "         'new_text':document_list[i].page_content})\n",
    "    print('')\n",
    "    print(intermediate_summary)\n",
    "    print('=======================')\n",
    "# 길이를 지정하지 않으면 오래 걸릴 수 있습니다.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
